---
title: How caching works in Next.js
nav_title: Caching
description: An deep dive into how caching works in Next.js
---

> This model is in active development, and subject to change.

Caching is a technique for storing the result of rendering, data fetching, and other computations so that future requests for the same data can be served faster, without doing the work again. In this deep dive, you'll learn how caching works in Next.js.

## Caching principles

In Next.js, caching is:

- [Client-first](#client-first-caching-with-server-first-rendering): The browser is the first place the framework checks before making new requests to the server.
- [Declarative](#declarative): You describe when a cache entry should be updated, not how it's stored.
- [Composable](#composable): You can granularly cache individual route segments, React Server Components, or regular functions.

### Client-first caching with server-first rendering

Next.js uses an unified model for client and server caching. When retrieving a cache entry, it checks:

1. The client cache
2. The server in-memory cache
3. A remote persistent cache, if defined

{/* TODO: Add diagram */}

#### Why client-first?

Some content—like personalized user information (based on `cookies`) or pages with high-cardinality params ([dynamic route segments]())-is _cacheable_ but not practical to prerender at build time or store in a persistent server cache.

For example, user profile data that doesn't change often. In the previous static-or-dynamic model, this content would be excluded from prerendering and treated as dynamic, re-rendering on every request. But, in Next.js, you can [mark it as cacheable](). This allows the content to be prefetched and cached on the client.

{/* TODO: Add diagram */}

This means more of the route is ready before navigation, improving speed and reducing server load. And only truly dynamic or frequently updated content is re-fetched on navigation.

However, **client-first does not mean client-only**. The server still plays a role in fetching and rendering, deduplicating requests, reusing static shells across users, and persisting static, shareable parts to a remote cache.

### Declarative

Caching in Next.js is declarative. You don't configure caching layers and infrastructure directly. Instead:

- You describe how long some data or UI can be reused, when it should be refreshed, and what triggers a revalidation.
- Next.js handles how and where it's cached-during build time or during runtime on [client](#client-cache), [server in-memory](#in-memory-caching-and-deduplication), or [remote persistent cache](#persistent-remote-caching).

### Composable

Next.js caching is granular and composable. You can cache route segments, nested Server Components, and individual functions. And [interleave](#interleaving-dynamic-and-static-content) static and dynamic content, only caching what's needed, while keeping dynamic or user-specific parts fresh.

This is helpful for:

1. Hybrid pages that combine static and dynamic parts.
2. High-cardinality params that generate many unique pages, making prerendering resource-intensive.
3. Personalized content based on user-specific data available only at request time.

## Caching APIs

The APIs used for caching are:

- [`"use cache"`](#how-use-cache-works): Marks a Server Component or function as cacheable.
- [`cacheLife`](#how-cacheLife-works): Sets when a cache entry should be updated.
- [`cacheTag`](#how-cacheTag-works) and [`revalidateTag`](#how-revalidateTag-works): Tag cache entries and update all entries with a matching tag.

## How `"use cache"` works

`"use cache"` is a Next.js directive that marks an async [Server Component](https://react.dev/reference/rsc/server-components) or function as cacheable. It creates a **boundary** around some inputs and outputs so you can describe when the output should update.

```tsx filename="app/page.tsx" switcher
export default async function Page() {
  'use cache'
  // ...
}
```

```jsx filename="app/page.js" switcher
export default async function Page() {
  'use cache'
  // ...
}
```

### How a cache entry is created

Functions marked with `"use cache"` are compiled into [Server Functions](https://react.dev/reference/rsc/server-functions).

Behind-the-scenes, Next.js:

1. Wraps the Server Function with caching semantics.
2. Generates an unique [cache key](#cache-keys).
3. Deduplicates identical calls using React's [`cache`](https://react.dev/reference/react/cache) function.
4. Stores the cache entry in [server memory](#in-memory-caching-and-deduplication) or a persistent, remote cache.
5. Annotates the Server Component payload with [#update-semantics](##update-semantics).
6. Sends the RSC payload to the client where the [client router]() will use the information to manage the cache.

> Jump to the [cache entry's lifecycle](#the-cache-entry-lifecycle) section to learn more about how Next.js determines when to update a cache entry.

### How are cache keys generated?

Each cache entry has a unique key, which is generated from:

- The build ID (generated for each build)
- The function signature (a secure identifier)
- Any [serializable](#serialization) arguments or props passed to the function
- Any values it reads from the parent scope

If the inputs passed to the function are different, a new cache entry is created. For example:

```tsx filename="app/lib/data.ts" switcher
// Same user ID = cache hit
// Different user ID = new entry

export async function getUser(id: string) {
  'use cache'
  return db.getUser(id)
}
```

```tsx filename="app/lib/data.ts" switcher
// Same user ID = cache hit
// Different user ID = new entry

export async function getUser(id: string) {
  'use cache'
  return db.getUser(id)
}
```

### Serialization

The return value of a cacheable function can be anything React can render. Concretely, it must be [serializable](https://react.dev/reference/rsc/use-server#serializable-parameters-and-return-values).

If you pass non-serializable values like functions or components, they are excluded from the [cache key](#cache-keys). Instead, they are treated as references and resolved at runtime. This allows [interleaving](#interleaving) dynamic and static content.

### Interleaving dynamic and static content

Interleaving lets you nest dynamic content inside cached components.

For example, you can mark a [layout](/docs/app/api-reference/file-conventions/layout) as cacheable while keeping its page (`{children}`) dynamic:

```tsx filename="app/dashboard/layout.tsx" switcher
function Layout({ children }: { children: React.ReactNode }) {
  'use cache'

  return (
    <div>
      {/* Navigation will be cached */}
      <Navigation />
      {/* Children can be dynamic */}
      {children}
    </div>
  )
}
```

```jsx filename="app/layout.js" switcher
function Layout({ children }: { children: React.ReactNode }) {
  'use cache'

  return (
    <div>
      {/* Navigation will be cached */}
      <Navigation />
      {/* Children can be dynamic */}
      {children}
    </div>
  )
}
```

The same applies for nested Server Components deeper in the tree:

```tsx filename="app/ui/profile.tsx" switcher
async function Profile({ id, children }) {
  'use cache'
  const user = await getUser(id)

  return (
    <>
      {/* Username is cached */}
      <h1>{user.name}</h1>
      {/* Children can be dynamic */}
      {children}
    </>
  )
}
```

```jsx filename="app/ui/profile.js" switcher
async function Profile({ id, children }) {
  'use cache'
  const user = await getUser(id)

  return (
    <>
      {/* Username is cached */}
      <h1>{user.name}</h1>
      {/* Children can be dynamic */}
      {children}
    </>
  )
}
```

Non-serializable values — like `{children}` props or dynamic nested components — are treated as references and not included in the [cache key](#cache-keys). Instead, these references are filled in at runtime. This prevents the dynamic values from “poisoning” the cache, which would otherwise generate a new cache entry for every variation.

## Server cache

### Deduplication and in-memory caching

By default, `"use cache"` is deduped and in-memory. These defaults:

- Speed up server rendering by accumulating cache entries during a pass, preventing the same function with the same inputs from being executed multiple times.
- Reduce requests to external database or APIs, reducing latency and costs.
- Work well as a simple and affordable starting point for [self-hosted](/docs/app/guides/self-hosting) applications.

#### How long does it last?

The in-memory cache only exists while the server is running.

- **Serverless**: the cache lasts for the lifetime of the serverless function.
- **Long-running server**: the cache lasts until the server is restarted or the memory is cleared.

### Persistent remote caching

> **Coming soon:** Persistent remote caching is under active development.

Unlike the default in-memory cache, a persistent remote cache stores entries in an external source. This allows cached entries to be reused across requests, servers, and deployments.

Cache entries persist for their [configured lifetime](#how-cachelife-works) or until they are [manually revalidated](#how-cacheTag-and-revalidateTag-works).

#### Defining a remote source

## Client cache

When a user visits the app for the first time, the client cache is populated with cache entries from the server response and background [prefetching]().

As the user navigates, the router checks the client cache:

- If a cache entry exists and is still fresh, navigation is instant.
- If the entry is missing or stale, the client sends a new request to the server.

Cache entries are stored in the browser's memory for the duration of the session or until the entry becomes become stale or are revalidated.

> **Good to know:** Client-side cache granularity is currently route-based. Segment-level granularity is coming soon and in the future we want to provide even more granular control of how long the client can reuse UI before checking the server.

[`cacheLife`](/docs/app/api-reference/functions/cacheLife) is a function that lets you define a cache entry's lifetime across the client and server. It should be used within the scope of the cacheable component or function:

## How `cacheLife` works

```tsx filename="app/page.tsx" switcher
import { unstable_cacheLife as cacheLife } from 'next/cache'

export default async function Page() {
  'use cache'
  cacheLife('hours')
  // ...
}
```

```jsx filename="app/page.js" switcher
import { unstable_cacheLife as cacheLife } from 'next/cache'

export default async function Page() {
  'use cache'
  cacheLife('hours')
  // ...
}
```

The function accepts a [predefined cache profile](/docs/app/api-reference/functions/cacheLife#default-cache-profiles) (e.g. `'seconds'`, `'minutes'`, `'hours'`, or a [custom one](/docs/app/api-reference/functions/cacheLife#custom-cache-profiles)). A `'use cache'` boundary that isn't annotated with `cacheLife` will use the `default` cache profile.

Profiles sets three properties:

- `stale`: when a cache entry on the client should be updated.
- `revalidate`: when the same cache entry on the server should be updated.
- `expire`: the maximum time a cached entry can be used before it expires.

These properties are used to define the cache entry's [lifecycle](#the-cache-entry-lifecycle).

### The cache entry's lifecycle

After a cache entry is stored on the client, Next.js determines when to update it based on its lifetime properties:

1. Once the `stale` period has passed, the client checks the server for updates.
2. If the entry is still within the `revalidate` period, the server returns the existing cached entry.
3. If the `revalidate` period has passed, the server immediately returns the stale entry and triggers a background revalidation, creating a fresh entry with extended lifetimes. The next request will receive the updated entry.
4. `expire` sets a hard limit on the server—once it passes, the server must generate a new entry before responding.

{/* TODO: Add lifecycle diagram */}

### High vs low traffic patterns

The model adapts to high and low traffic patterns on the server.

During **low traffic**, requests are rare, so cache entries might become stale or expire. Revalidating on a fixed time schedule would be wasteful for rarely accessed data. Instead, a revalidation is triggered only when a new request comes in.

{/* TODO: Add diagram */}

During **high traffic**, requests are frequent, the server is shielded from excessive re-executions while the cache entry is fresh. At the same time, frequent requests signal that the cached function is important and shouldn't be stale for a long time. By tying revalidation to the incoming request, this ensures the cache entry is updated soon after it becomes stale.

{/* TODO: Add diagram */}

> The difference between `revalidate` and `expire` is that `revalidate` immediately responds with a stale entry and revalidates in the background, whereas `expire` blocks the response until the new entry is generated.
